{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNNL4bM+XuwfMV3+W685mpf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sankalp294/Combat-Solutions-tasks/blob/main/2_langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOUQ8VYtgeod",
        "outputId": "c45a6365-9cea-46ba-c282-8f632be3ca37"
      },
      "source": [
        "!pip install -q langchain langchain-core langchain-groq langgraph langchain-community\n",
        "\n",
        "\n",
        "import os\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "from langgraph.graph import StateGraph, END\n",
        "from typing import TypedDict\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = \" xyz \"\n",
        "\n",
        "\n",
        "#Simple LangChain Pipeline\n",
        "print(\"=\" * 60)\n",
        "print(\"PART 1: LangChain Pipeline\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "#PromptTemplate\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"history\", \"input\"],\n",
        "    template=\"\"\"Previous conversation: {history}\n",
        "User: {input}\n",
        "Assistant:\"\"\"\n",
        ")\n",
        "\n",
        "#Model\n",
        "llm = ChatGroq(\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "chat_history = ChatMessageHistory()\n",
        "chain = prompt | llm\n",
        "\n",
        "print(\"\\nConversation 1:\")\n",
        "user_input = \"Hi! My name is Sankalp\"\n",
        "history = \"\\n\".join([f\"{msg.type}: {msg.content}\" for msg in chat_history.messages])\n",
        "response = chain.invoke({\"history\": history, \"input\": user_input}).content\n",
        "print(response)\n",
        "chat_history.add_user_message(user_input)\n",
        "chat_history.add_ai_message(response)\n",
        "\n",
        "print(\"\\nConversation 2:\")\n",
        "user_input = \"What's my name?\"\n",
        "history = \"\\n\".join([f\"{msg.type}: {msg.content}\" for msg in chat_history.messages])\n",
        "response = chain.invoke({\"history\": history, \"input\": user_input}).content\n",
        "print(response)\n",
        "chat_history.add_user_message(user_input)\n",
        "chat_history.add_ai_message(response)\n",
        "\n",
        "print(\"\\nConversation 3:\")\n",
        "user_input = \"Tell me a joke.\"\n",
        "history = \"\\n\".join([f\"{msg.type}: {msg.content}\" for msg in chat_history.messages])\n",
        "response = chain.invoke({\"history\": history, \"input\": user_input}).content\n",
        "print(response)\n",
        "\n",
        "\n",
        "# Basic LangGraph Flow\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"PART 2: LangGraph Flow\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Define State\n",
        "class State(TypedDict):\n",
        "    input: str\n",
        "    output: str\n",
        "\n",
        "# Input Node\n",
        "def input_node(state: State):\n",
        "    print(f\"[INPUT] {state['input']}\")\n",
        "    return state\n",
        "\n",
        "# LLM Node\n",
        "def llm_node(state: State):\n",
        "    print(\"[LLM] Processing...\")\n",
        "    llm = ChatGroq(model=\"llama-3.3-70b-versatile\")\n",
        "    response = llm.invoke(state[\"input\"]).content\n",
        "    state[\"output\"] = response\n",
        "    return state\n",
        "\n",
        "# Output Node\n",
        "def output_node(state: State):\n",
        "    print(f\"[OUTPUT] {state['output']}\")\n",
        "    return state\n",
        "\n",
        "# Graph\n",
        "workflow = StateGraph(State)\n",
        "workflow.add_node(\"input\", input_node)\n",
        "workflow.add_node(\"llm\", llm_node)\n",
        "workflow.add_node(\"output\", output_node)\n",
        "\n",
        "workflow.set_entry_point(\"input\")\n",
        "workflow.add_edge(\"input\", \"llm\")\n",
        "workflow.add_edge(\"llm\", \"output\")\n",
        "workflow.add_edge(\"output\", END)\n",
        "\n",
        "app = workflow.compile()\n",
        "\n",
        "#Run Graph\n",
        "result = app.invoke({\"input\": \"What are the primary colors?\", \"output\": \"\"})\n",
        "\n",
        "print(\"\\n--- Result ---\")\n",
        "print(f\"Question: {result['input']}\")\n",
        "print(f\"Answer: {result['output']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\" Done! Both examples completed successfully.\")\n",
        "print(\"=\" * 60)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m============================================================\n",
            "PART 1: LangChain Pipeline\n",
            "============================================================\n",
            "\n",
            "Conversation 1:\n",
            "Hello Sankalp, it's nice to meet you. Is there something I can help you with or would you like to chat?\n",
            "\n",
            "Conversation 2:\n",
            "Your name is Sankalp. We just introduced ourselves at the beginning of our conversation.\n",
            "\n",
            "Conversation 3:\n",
            "Here's one: Why couldn't the bicycle stand up by itself? Because it was two-tired. I hope that made you laugh, Sankalp! Do you want to hear another one?\n",
            "\n",
            "============================================================\n",
            "PART 2: LangGraph Flow\n",
            "============================================================\n",
            "[INPUT] What are the primary colors?\n",
            "[LLM] Processing...\n",
            "[OUTPUT] The primary colors are:\n",
            "\n",
            "1. Red\n",
            "2. Blue\n",
            "3. Yellow\n",
            "\n",
            "These colors cannot be created by mixing other colors together, and they are the base colors used to create all other colors.\n",
            "\n",
            "--- Result ---\n",
            "Question: What are the primary colors?\n",
            "Answer: The primary colors are:\n",
            "\n",
            "1. Red\n",
            "2. Blue\n",
            "3. Yellow\n",
            "\n",
            "These colors cannot be created by mixing other colors together, and they are the base colors used to create all other colors.\n",
            "\n",
            "============================================================\n",
            " Done! Both examples completed successfully.\n",
            "============================================================\n"
          ]
        }
      ]
    }
  ]
}